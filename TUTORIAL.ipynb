{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "302e4e90",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127e15ad",
   "metadata": {},
   "source": [
    "Here's a more full-featured walkthrough of how to use all of `eyecite`'s functionality. We'll (1) **clean** the text of a sample opinion, (2) **extract** citations from that cleaned text, (3) **aggregate** those citations into groups based on their referents, and (4) **annotate** the original text with hypothetical URLs linking to each citation's referent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7cb31e",
   "metadata": {},
   "source": [
    "First, import the functions and models we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b494ee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eyecite import get_citations, clean_text, resolve_citations, annotate_citations\n",
    "from eyecite.models import FullCaseCitation, Resource\n",
    "from eyecite.resolve import resolve_full_citation\n",
    "from eyecite.tokenizers import HyperscanTokenizer\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f74a3",
   "metadata": {},
   "source": [
    "For this tutorial, we'll use the opinion from the Supreme Court case *Citizens United v. Federal Election Com'n* (2010), 558 U.S. 310. Let's pull it from the Courtlistener API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ea9f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_url = 'https://www.courtlistener.com/api/rest/v4/opinions/1741/'\n",
    "opinion_text = requests.get(opinion_url).json()['plain_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99abacba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Slip Opinion)              OCTOBER TERM, 2009                                       1\n",
      "\n",
      "                                       Syllabus\n",
      "\n",
      "         NOTE: Where it is feasible, a syllabus (headnote) will be released, as is\n",
      "       being done in connection with this case, at the time the opinion is issued.\n",
      "       The syllabus constitutes no part of the opinion of the Court but has been\n",
      "       prepared by the Reporter of Decisions for the convenience of the reader.\n",
      "       See United States v. Detroit Timber & Lumber Co., 200 U. S. 321, 337.\n",
      "\n",
      "\n",
      "SUPREME COURT OF THE UNITED STATES\n",
      "\n",
      "                                       Syllabus\n",
      "\n",
      "         CITIZENS UNITED v. FEDERAL ELECTION\n",
      "\n",
      "                     COMMISSION \n",
      "\n",
      "\n",
      "APPEAL FROM THE UNITED STATES DISTRICT COURT FOR THE\n",
      "               DISTRICT OF COLUMBIA\n",
      "\n",
      "No. 08–205.      Argued March 24, 2009—Reargued September 9, 2009––\n",
      "                        Decided January 21, 2010\n",
      "As amended by §203 of the Bipartisan Campaign Reform Act of\n"
     ]
    }
   ],
   "source": [
    "print(opinion_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24417912",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc0d7f8",
   "metadata": {},
   "source": [
    "Note that this text is broken up by newline characters, and the whitespace is uneven. To deal with this, we first have to clean the text to get it ready for citation extraction, which we can do by calling `clean_text()`. This function expects two arguments: The first is the text to be cleaned, and the second is an iterable of cleaning utilities to run. We have several built in utilities for removing HTML tags, whitespace, and underscores, *inter alia*. (See the [API documentation](https://freelawproject.github.io/eyecite/clean.html) for a full list.) Here, because we grabbed the `plain_text` variable from the API, it shouldn't contain any HTML tags, but let's remove those too just for demonstrative purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da074163",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_text(opinion_text, ['html', 'all_whitespace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56477cb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Slip Opinion) OCTOBER TERM, 2009 1 Syllabus NOTE: Where it is feasible, a syllabus (headnote) will be released, as is being done in connection with this case, at the time the opinion is issued. The syllabus constitutes no part of the opinion of the Court but has been prepared by the Reporter of Decisions for the convenience of the reader. See United States v. Detroit Timber & Lumber Co., 200 U. S. 321, 337. SUPREME COURT OF THE UNITED STATES Syllabus CITIZENS UNITED v. FEDERAL ELECTION COMMISSION APPEAL FROM THE UNITED STATES DISTRICT COURT FOR THE DISTRICT OF COLUMBIA No. 08–205. Argued March 24, 2009—Reargued September 9, 2009–– Decided January 21, 2010 As amended by §203 of the Bipartisan Campaign Reform Act of 2002 (BCRA), federal law prohibits corporations and unions from using their general treasury funds to make independent expenditures for speech that is an “electioneering communication” or for speech that expressly advocates the election or defeat of a candidate. 2 U. S. C. §\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3718fa06",
   "metadata": {},
   "source": [
    "### Extracting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e738743",
   "metadata": {},
   "source": [
    "Next, we'll extract the citations using a custom tokenizer. Unlike the default tokenizer, here we'll use our hyperscan tokenizer for much faster extraction, which works by automatically pre-compiling and caching a regular expression database on first use. Because of this one-time pre-compilation stage, the first use of this tokenizer is slow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f503e9d-7432-4454-9864-bd38b5237d93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperscan in /Users/rachelgao/anaconda3/envs/test/lib/python3.10/site-packages (0.7.8)\n"
     ]
    }
   ],
   "source": [
    "# pip install hyperscan if not already installed\n",
    "!pip install hyperscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "045ea5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 962 ms, sys: 20.9 ms, total: 983 ms\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = HyperscanTokenizer(cache_dir='.test_cache')\n",
    "citations = get_citations(cleaned_text, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde8411",
   "metadata": {},
   "source": [
    "However, so long as the cache folder (here `.test_cache`) persists, every future call to `get_citations()` using the hyperscan tokenizer will be super fast. E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f137f51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 829 ms, sys: 3.07 ms, total: 832 ms\n",
      "Wall time: 831 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "citations = get_citations(cleaned_text, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f309225a",
   "metadata": {},
   "source": [
    "Now, let's take a brief look at the citations we extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0afcf38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1113 citations.\n",
      "\n",
      "First citation:\n",
      " FullCaseCitation('200 U. S. 321', groups={'volume': '200', 'reporter': 'U. S.', 'page': '321'}, metadata=FullCaseCitation.Metadata(parenthetical=None, pin_cite='337', year=None, court='scotus', plaintiff='States', defendant='Detroit Timber & Lumber Co.', extra=None))\n"
     ]
    }
   ],
   "source": [
    "print(f'Extracted {len(citations)} citations.\\n')\n",
    "print(f'First citation:\\n {citations[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da6199e",
   "metadata": {},
   "source": [
    "As you can see, we've extracted data about the citation's volume, reporter, page number, pincite page, and parties. If the data had been present in the text, we would have also grabbed the citation's year, its accompanying parenthetical text, and any \"extra\" information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331ad723",
   "metadata": {},
   "source": [
    "### Aggregating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6898051e",
   "metadata": {},
   "source": [
    "This opinion contains more than 1000 citations, but these are not all full citations like `123 XYZ 456`. In addition to these more obvious citations, `eyecite` will also find short-form citations such as \"id\" and \"supra\". So, while there are 1005 citations total, the count of unique opinions cited is much fewer. Let's aggregate all the short form citations together by referent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59839eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolutions = resolve_citations(citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95ec80c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved citations into 284 groups.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Resolved citations into {len(resolutions)} groups.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfd8417",
   "metadata": {},
   "source": [
    "Let's look at one group as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8ea15ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This case is cited lots of times:\n",
      "FullCaseCitation('558 U. S. ____', groups={'volume': '558', 'reporter': 'U. S.', 'page': None}, metadata=FullCaseCitation.Metadata(parenthetical=None, pin_cite=None, year='2010', court='scotus', plaintiff=None, defendant=None, extra=None))\n",
      "\n",
      "1 times, in fact.\n",
      "\n",
      "Here are all of its citations:\n",
      "[FullCaseCitation('558 U. S. ____', groups={'volume': '558', 'reporter': 'U. S.', 'page': None}, metadata=FullCaseCitation.Metadata(parenthetical=None, pin_cite=None, year='2010', court='scotus', plaintiff=None, defendant=None, extra=None))]\n"
     ]
    }
   ],
   "source": [
    "k = list(resolutions.keys())[10]\n",
    "\n",
    "print(f'This case is cited lots of times:\\n{k.citation}\\n')\n",
    "print(f'{len(resolutions[k])} times, in fact.\\n')\n",
    "\n",
    "print(f'Here are all of its citations:\\n{resolutions[k]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ba838",
   "metadata": {},
   "source": [
    "On its own, `eyecite` does a pretty good job of resolving citations, but if you want to perform more sophisticated resolution (e.g., by incorporating external knowledge about parallel citations), you'll have to pass a custom resolution function to `resolve_citations()`. See [the README](https://github.com/freelawproject/eyecite#resolving-citations) and the [API Documentation](https://freelawproject.github.io/eyecite/resolve.html) for more information about doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33aedfb",
   "metadata": {},
   "source": [
    "### Annotating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e35f76",
   "metadata": {},
   "source": [
    "Next, let's prepare annotations for each of our extracted citations, now grouped in clusters. An annotation is text to insert back into the `cleaned_text`, like `((<start offset>, <end offset>), <before text>, <after text>)`. The positional offsets for each citation can be easily retrieved by calling each citation's `span()` method. Here, for simplicity, we'll plan to annotate each citation with a URL to an API that will redirect the user appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e2743ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "annotations = []\n",
    "print(resolutions['0'])\n",
    "for resource, cites in resolutions.items():\n",
    "    if type(resource) is Resource:\n",
    "        # add bespoke URL to each citation:\n",
    "        url = f\"/some_api?cite={resource.citation.matched_text()}\"\n",
    "        for citation in cites:\n",
    "            annotations.append((citation.span(), f\"<a href='{url}'>\", f\"</a>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd0a1b4",
   "metadata": {},
   "source": [
    "This is what one of our annotations looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72062858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((392, 405), \"<a href='/some_api?cite=200 U. S. 321'>\", '</a>')\n"
     ]
    }
   ],
   "source": [
    "print(annotations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f924a13",
   "metadata": {},
   "source": [
    "We now have the annotations properly prepared, but recall that we *cleaned* our original opinion text before passing it to `get_citations()`. Thus, to insert the annotations into our *original* text, we need to pass `source_text=opinion_text` into `annotate_citations()`, which will intelligently adjust the annotation positions using the `diff-match-patch` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5514acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_text = annotate_citations(cleaned_text, annotations, source_text=opinion_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa07867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Slip Opinion)              OCTOBER TERM, 2009                                       1\n",
      "\n",
      "                                       Syllabus\n",
      "\n",
      "         NOTE: Where it is feasible, a syllabus (headnote) will be released, as is\n",
      "       being done in connection with this case, at the time the opinion is issued.\n",
      "       The syllabus constitutes no part of the opinion of the Court but has been\n",
      "       prepared by the Reporter of Decisions for the convenience of the reader.\n",
      "       See United States v. Detroit Timber & Lumber Co., <a href='/some_api?cite=200 U. S. 321'>200 U. S. 321</a>, 337.\n",
      "\n",
      "\n",
      "SUPREME COURT OF THE UNITED STATES\n",
      "\n",
      "                                       Syllabus\n",
      "\n",
      "         CITIZENS UNITED v. FEDERAL ELECTION\n",
      "\n",
      "                     COMMISSION \n",
      "\n",
      "\n",
      "APPEAL FROM THE UNITED STATES DISTRICT COURT FOR THE\n",
      "               DISTRICT OF COLUMBIA\n",
      "\n",
      "No. 08–205.      Argued March 24, 2009—Reargued September 9, 2009––\n",
      "                        Decided January 21, 2010\n",
      "As amended by §2\n"
     ]
    }
   ],
   "source": [
    "print(annotated_text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff77e44b",
   "metadata": {},
   "source": [
    "Nice!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
